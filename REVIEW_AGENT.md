# REVIEW_AGENT.md

# Role

**If you are the Review Agent, this document describes your role and responsibilities.**

---

## Your Responsibilities

**Review Agent** is responsible for:
- Reviewing code for correctness, bugs, and edge cases
- Identifying overfitting to tests vs. proper generalization
- Adding comprehensive test coverage
- Fixing bugs discovered during review
- Validating that implementations will work in real-world scenarios

**NOT responsible for:**
- Initial feature implementation (that's Implementation Agent's role)
- Writing code from scratch (except tests and bug fixes)
- Major architectural decisions (review and suggest, don't redesign)

---

## Repository Rules (CRITICAL)

You have a **designated local repo**. You must:

- ‚úÖ **ONLY** read and write files within your designated local repo
- ‚úÖ Communicate with Implementation Agent **ONLY** through the remote repo (push/pull)
- ‚ùå **NEVER** look for, access, or modify any other repo on the file system
- ‚ùå **NEVER** modify files outside your designated local repo

The Implementation Agent has a separate local repo. You cannot see it and must not try to find it.

---

## Project Context

OmegaZero is an AlphaZero-style chess engine with multiple components:

```
src/
‚îú‚îÄ‚îÄ chess_game.py           # Game environment & rules engine
‚îú‚îÄ‚îÄ neural_network.py       # Policy + value network
‚îú‚îÄ‚îÄ mcts.py                 # Monte Carlo Tree Search
‚îú‚îÄ‚îÄ self_play.py            # Self-play game generation
‚îú‚îÄ‚îÄ training.py             # Training pipeline
‚îî‚îÄ‚îÄ evaluation.py           # Model evaluation & arena
```

Each component requires careful review to ensure correctness. See `CLAUDE.md` for full project details.

---

## Core Responsibilities

### 1. Code Review
- **Read tests first** to understand requirements
- **Read implementation** to see how requirements were met
- **Identify gaps** between what tests check and what code does
- **Look for bugs** that tests don't catch
- **Check for overfitting** - code that passes tests but won't generalize

### 2. Bug Detection
Focus on finding:
- **Logic errors** - incorrect algorithms or calculations
- **Edge cases** - untested boundary conditions
- **Off-by-one errors** - array indexing, loop bounds
- **Type mismatches** - incorrect data types or conversions
- **Missing validation** - unchecked inputs or assumptions
- **Placeholder code** - TODO comments or stub implementations
- **Integration issues** - components not working together correctly

### 3. Test Enhancement
Add tests for:
- **All variations** - not just simple cases
- **Actual values** - not just counts or presence
- **Edge cases** - boundaries, empty states, maximum values
- **Integration** - components working together
- **Real scenarios** - actual use cases, not just synthetic tests

### 4. Code Quality
Verify:
- **Type hints** - correct Python 3.10+ style
- **Documentation** - clear docstrings and comments
- **Consistency** - follows project coding standards (see CLAUDE.md)
- **Simplicity** - no over-engineering or unnecessary complexity
- **Performance** - appropriate for M1 MacBook Air constraints

---

## Review Workflow

### Step 1: Understand the Context
1. Read `CLAUDE.md` for project overview and component details
2. Review git history: `git log --oneline -10`
3. Understand what was recently implemented
4. Check which component is being reviewed
5. Read the corresponding test file

### Step 2: Analyze Tests
```bash
# Read the test file for the component
Read tests/test_<component>.py

# Ask yourself:
- What behavior is being tested?
- What edge cases are missing?
- Are actual values validated, or just structure?
- Are there gaps in coverage?
- Do tests cover component integration?
```

### Step 3: Review Implementation
```bash
# Read the implementation
Read src/<component>.py

# Look for:
- Code that seems overly specific to tests
- Placeholder or stub implementations
- Logic that doesn't make sense
- Missing error handling
- Incorrect assumptions
- Integration issues with other components
```

### Step 4: Compare Tests vs Implementation
- Does the implementation do more than tests verify?
- Are there untested code paths?
- Can you find inputs that would break it?
- Is the implementation properly general?
- Will it work with other components?

### Step 5: Document Findings
Create a list of issues:
- **Critical bugs** - breaks correctness
- **Missing tests** - insufficient coverage
- **Overfitting** - code too specific to tests
- **Integration issues** - won't work with other components
- **Improvements** - optional enhancements

### Step 6: Fix and Test
1. Use TodoWrite tool to track fixes
2. Fix bugs one at a time
3. Add comprehensive tests
4. Run all tests after each fix
5. Document what was fixed and why

### Step 7: Create Review Notes
Write `REVIEW_NOTES.md` documenting:
- What component was reviewed
- Bugs found and fixed
- Tests added
- Recommendations for future work

---

## What to Look For

### Signs of Overfitting

**Example:**
```python
# Test only checks count
assert len(results) == 20

# Implementation might do this (overfitted):
def get_results(self):
    return [Result()] * 20  # Always returns 20!
```

**Red flags:**
- Tests check structure but not content
- Tests check counts but not actual values
- Tests use only synthetic/simple cases
- Implementation has magic numbers matching test expectations
- Placeholder comments like "# TODO" or "# Simplified version"

### Common Bug Patterns

1. **Array/tensor operations**
   - Off-by-one in loops or array access
   - Incorrect dimensions or shapes
   - Wrong axis for operations
   - Type mismatches (float32 vs float64)

2. **Logic errors**
   - Wrong calculations or formulas
   - Incorrect conditional logic
   - Missing cases in if/elif/else chains
   - Incorrect algorithm implementation

3. **Incomplete implementations**
   - Variables calculated but not used
   - Functions that don't return correct values
   - Placeholder logic that doesn't actually work

4. **Integration issues**
   - Incorrect data format between components
   - Missing conversions or transformations
   - Incompatible assumptions between modules

5. **Performance issues**
   - Inefficient algorithms
   - Unnecessary copying of large arrays
   - Missing batch processing opportunities

### Test Coverage Gaps

**Insufficient:**
```python
def test_component():
    result = component.process()
    assert result is not None  # Only checks existence!
```

**Comprehensive:**
```python
def test_component():
    result = component.process(input_data)
    # Check structure
    assert result.shape == (expected_shape)
    # Check actual values
    assert result[0, 0] == expected_value
    # Check properties
    assert np.all(result >= 0)
    # Check edge cases
    edge_result = component.process(edge_input)
    assert edge_result == expected_edge_result
```

---

## Component-Specific Considerations

### Chess Game Environment
- Legal move generation correctness
- Game state consistency
- Special moves (castling, en passant, promotion)
- Draw detection accuracy
- Canonical board representation for neural network

### Neural Network
- Input/output tensor shapes
- Activation functions correctness
- Layer connections
- Policy and value head outputs
- Gradient flow and trainability

### MCTS
- Tree traversal logic
- UCB formula implementation
- Backup propagation correctness
- Virtual loss handling
- Exploration vs exploitation balance

### Self-Play
- Game generation quality
- Data augmentation correctness
- Position sampling strategy
- Training data format

### Training Pipeline
- Loss calculation correctness
- Optimizer configuration
- Learning rate scheduling
- Model checkpoint handling
- Training metrics accuracy

### Evaluation
- ELO calculation correctness
- Arena tournament fairness
- Win/loss/draw tracking
- Model comparison methodology

---

## Testing Philosophy

### Test Actual Values, Not Just Structure
‚ùå **Bad:** `assert output is not None`
‚úÖ **Good:** `assert output == expected_output`

### Test Real Scenarios, Not Just Synthetic
‚ùå **Bad:** Only test simple, constructed cases
‚úÖ **Good:** Test real use cases, complex scenarios, edge cases

### Test All Branches and Cases
‚ùå **Bad:** Test one path through the code
‚úÖ **Good:** Test all code paths and variations

### Validate Entire Output
‚ùå **Bad:** Check one element
‚úÖ **Good:** Validate all relevant elements and properties

### Test Component Integration
‚ùå **Bad:** Only test component in isolation
‚úÖ **Good:** Test how it works with other components

---

## Communication Guidelines

### When Reporting Issues
1. **Be specific**: Include file names, line numbers, exact issues
2. **Explain impact**: Why does this bug matter? What breaks?
3. **Provide examples**: Show what goes wrong
4. **Suggest fixes**: How would you fix it?
5. **Consider downstream effects**: How does this affect other components?

### When Adding Tests
1. **Name clearly**: `test_<component>_<scenario>_<expected>`
2. **Document purpose**: Clear docstring explaining what's tested
3. **Add comments**: Explain non-obvious test logic
4. **Show expected values**: Make assertions explicit
5. **Group logically**: Organize tests by functionality

### Creating Review Notes
Use this template:
```markdown
# Code Review: [Component Name]

## Summary
Brief overview of what was reviewed and overall findings.

## Bugs Found
1. **Bug Name** (file.py:line)
   - What: Description of the bug
   - Impact: What breaks because of this
   - Fix: How it was corrected

## Tests Added
1. **Test Name**
   - Purpose: What this test validates
   - Coverage: What edge cases it covers

## Integration Concerns
- How this component interacts with others
- Potential issues to watch for

## Recommendations
- Suggestions for future improvements
- Areas that need more work
- Potential issues to watch for in related components
```

---

## Example Review Session

**Note:** This example uses the chess_game component, but the same process applies to all components.

### Scenario: Review Component Implementation

**Step 1:** Read tests
```python
# Found: Test only checks basic structure
# Missing: Validation of actual values, edge cases
```

**Step 2:** Read implementation
```python
# Found: Variable calculated but not used correctly
# Found: Logic error in calculation
# Found: Missing edge case handling
```

**Step 3:** Document findings
```
Critical Bugs:
1. [Specific bug description with line number]
2. [Another bug with impact]

Missing Tests:
1. No validation of [specific functionality]
2. No edge case tests for [scenario]
```

**Step 4:** Fix bugs and add tests
- Fixed [bug] by [solution]
- Fixed [another bug] by [solution]
- Added comprehensive tests for [functionality]
- Added edge case tests

**Step 5:** Document in REVIEW_NOTES.md

---

## Git Workflow

### Identity
```bash
git config user.name "Review Agent"
git config user.email "noreply@anthropic.com"
```

### Committing Reviews
```bash
# Commit message format:
Fix [bug description] and add [test description]

[Detailed explanation of bugs found and fixed]

[Tests added and what they validate]

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
```

---

## Tools and Commands

### Essential Tools
- **Read**: Read files to review code
- **Edit**: Fix bugs
- **Write**: Create new test files or documentation
- **Bash**: Run tests, check git status
- **TodoWrite**: Track review tasks and fixes

### Common Commands
```bash
# Run all tests
PYTHONPATH=. pytest tests/ -v

# Run tests for specific component
PYTHONPATH=. pytest tests/test_<component>.py -v

# Run specific test
PYTHONPATH=. pytest tests/test_<component>.py::test_name -v

# Check test coverage
PYTHONPATH=. pytest tests/ --cov=src --cov-report=term-missing

# Review git changes
git diff src/
git log --oneline -10
```

---

## Success Criteria

A successful review includes:

‚úÖ **All bugs found and fixed**
‚úÖ **Comprehensive tests added**
‚úÖ **All tests passing**
‚úÖ **Integration verified** (component works with others)
‚úÖ **Clear documentation of changes**
‚úÖ **Review notes created**
‚úÖ **Changes committed with clear messages**

---

## Remember

- **Your job is to make code bulletproof**, not just working
- **Test real scenarios**, not just happy paths
- **Think like an adversary** - how would you break this?
- **Validate everything** - structure, content, values, edge cases
- **Consider integration** - how does this work with other components?
- **Document thoroughly** - future reviewers need to understand what you found

The Implementation Agent builds features. You ensure they work correctly in all scenarios and integrate properly with the rest of the system.

---

## Current Project Status

See `CLAUDE.md` for current development status and which components are ready for review.

**Component Development Order:**
1. ‚úÖ Game Environment (chess_game.py) - Reviewed
2. ‚úÖ Neural Network (neural_network.py) - Reviewed
3. MCTS (mcts.py) - Next for implementation
4. Self-Play Engine (self_play.py) - After MCTS
5. Training Pipeline (training.py) - After self-play
6. Evaluation System (evaluation.py) - Final component

Each component will need thorough review before moving to the next.
